# RAG System for Financial & Legal Question Answering (Apple & Tesla 10-Ks)

## ğŸ“Œ Project Overview

This project implements a Retrieval-Augmented Generation (RAG) system that answers complex financial and legal questions using Apple Inc.'s FY2024 Form 10-K and Tesla, Inc.'s FY2023 Form 10-K filings.

The system retrieves relevant sections from the filings and generates accurate, well-sourced answers using open-source embedding models and LLMs, without relying on any closed or proprietary APIs.

## ğŸ“‚ Input Documents

The following PDFs were provided as part of the assignment and used directly:

- **Apple Inc. Form 10-K** (Fiscal year ended September 28, 2024)
- **Tesla, Inc. Form 10-K** (Fiscal year ended December 31, 2023)

## ğŸ§  System Architecture

```
PDF Documents
    â†“
PDF Parsing (page-level)
    â†“
Section (Item) Detection
    â†“
Chunking with Overlap
    â†“
Embedding Generation (Open Source)
    â†“
Vector Database (ChromaDB)
    â†“
Similarity Search (Top-K)
    â†“
Cross-Encoder Re-Ranking
    â†“
LLM Answer Generation (Context-only)
    â†“
JSON Answer with Citations
```

## ğŸ§© Key Features

- Page-aware and section-aware citations
- Metadata-preserving chunking
- Open-source embeddings and LLMs only
- Strict hallucination prevention
- Clear refusal handling for out-of-scope queries
- Separate indexing for Apple and Tesla filings

## ğŸ› ï¸ Technology Stack

| Component | Tool |
|-----------|------|
| PDF Parsing | pypdf |
| Embeddings | sentence-transformers/all-MiniLM-L6-v2 |
| Vector Database | ChromaDB |
| Re-Ranking | Cross-Encoder (ms-marco-MiniLM) |
| LLM | Mistral / LLaMA 3 (open-access) |
| Runtime | Python 3.10+, Google Colab / Kaggle |

## ğŸ“ Project Structure

```
rag-10k-system/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 10-Q4-2024-As-Filed.pdf
â”‚   â””â”€â”€ tsla-20231231-gen.pdf
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ section_parser.py
â”‚   â””â”€â”€ chunker.py
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â””â”€â”€ vector_store.py
â”‚
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ retriever.py
â”‚   â””â”€â”€ reranker.py
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ prompt.py
â”‚   â””â”€â”€ generator.py
â”‚
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ rag_pipeline.py
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ run_rag.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ design_report.md
```

## ğŸ”‘ Required Interface

The system exposes a single callable function:

```python
answer_question(query: str) â†’ dict
```

**Returns:**

- `answer`: the generated answer or a refusal message
- `sources`: list of citations in the format `["Apple 10-K", "Item 8", "p. 282"]`

## ğŸš€ How to Run (Colab / Kaggle)

1. Clone the GitHub repository
2. Install dependencies using requirements.txt
3. Run the notebook `notebook/run_rag.ipynb`
4. Use `answer_question(query)` to query the system

## âš ï¸ Notes

- Closed APIs such as GPT-4 or Claude are not used.
- The LLM is strictly limited to retrieved context.
- Some early document pages do not belong to any Item section and are labeled as "Unknown".

## âœ… Compliance Summary

- âœ” Open-source models only
- âœ” Context-grounded answers
- âœ” Mandatory citations
- âœ” Explicit out-of-scope refusal
- âœ” Fully reproducible notebook
